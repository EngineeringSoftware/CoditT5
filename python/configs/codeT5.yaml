data:
  batch_size: 1
  eval_batch_size: 4

model:
  pretrained_model: Salesforce/codet5-base
  pretrained_tokenizer: ../../models/codeT5Tokenizer
  beam_size: 20

trainer:
  auto_select_gpus: true
  gpus: -1
  strategy: ddp
  precision: 16

  max_epochs: 50
  accumulate_grad_batches: 12 # effective batch size 1*4(gpu)*12(accumulate) = 48

  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: bleu/val
        mode: max
        min_delta: 0
        patience: 5
        verbose: true
    - class_path: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor
      init_args:
        logging_interval: step

optimizer:
  class_path: transformers.optimization.AdamW
  init_args:
    lr: 0.00005
    eps: 1e-8
    weight_decay: 0.01

lr_scheduler:
  class_path: torch.optim.lr_scheduler.OneCycleLR
  init_args:
    max_lr: 0.00005
    pct_start: 0.1
    div_factor: 1
    total_steps: 50
    anneal_strategy: linear

ckpt:
  save_top_k: 1
  monitor: bleu/val
  mode: max
